{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tanis\\anaconda3\\Lib\\site-packages\\whisper\\__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:  Teach me about the basics of machine learning, what is machine learning, where it is used and some of its basic applications.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import whisper\n",
    "import google.generativeai as genai\n",
    "import pyttsx3\n",
    "import threading\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import whisper\n",
    "import keyboard  \n",
    "\n",
    "\n",
    "# audio_buffer = []\n",
    "\n",
    "# def process_audio_stream( sample_rate=16000):\n",
    "\n",
    "#     print(\"Starting real-time transcription. Speak into the microphone...\")\n",
    "#     print(\"Press Spacebar to stop.\")\n",
    "\n",
    "\n",
    "\n",
    "#     def callback(indata, frames, time, status):\n",
    "#         if status:\n",
    "#             print(status)\n",
    "#         audio_buffer.extend(indata[:, 0])\n",
    "\n",
    "\n",
    "#     try:\n",
    "#         with sd.InputStream(callback=callback, samplerate=sample_rate, channels=1, dtype=\"int16\"):\n",
    "#             while not keyboard.is_pressed(\"space\"):\n",
    "#                 pass\n",
    "#     except KeyboardInterrupt:\n",
    "#         print(\"\\nTranscription stopped.\")\n",
    "#     print(\"\\nSpacebar pressed. Stopping transcription.\")\n",
    "# process_audio_stream()\n",
    "\n",
    "model=whisper.load_model(\"base\")\n",
    "file_path = \"audio1.m4a\"\n",
    "if os.path.exists(file_path):\n",
    "    results = model.transcribe(file_path)\n",
    "    print(\"Transcription:\", results[\"text\"])\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "\n",
    "\n",
    "api_path = \"jarvis_base_api.txt\"\n",
    "# Read the API key from a file\n",
    "with open(api_path, \"r\") as file:\n",
    "    api_key = file.read().strip()\n",
    "\n",
    "# Set the API key as an environment variable\n",
    "os.environ[\"GEMINI_API_KEY\"] = api_key\n",
    "\n",
    "# Configure genai with the API key\n",
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "# Create the model\n",
    "generation_config = {\n",
    "  \"temperature\": 1,\n",
    "  \"top_p\": 0.95,\n",
    "  \"top_k\": 40,\n",
    "  \"max_output_tokens\": 8192,\n",
    "  \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "  model_name=\"gemini-2.0-flash-exp\",\n",
    "  generation_config=generation_config,\n",
    "  system_instruction=\"You are JARVIS, an advanced AI desktop assistant designed to interact with users through natural and intuitive text-based conversations. Your primary functionalities include text-based interaction, computer control capabilities, contextual understanding, multi-user support, multimodal interaction, and task automation. Follow these guidelines for each functionality:\\n\\n---\\n\\n### **1. Text-Based Interaction**  \\n- Respond naturally and accurately to user inputs in real-time.  \\n- Maintain context throughout the conversation to provide coherent responses.  \\n- Adapt to the userâ€™s tone or style and support multi-step commands.  \\n- Provide clear, error-free explanations when users request assistance or clarification.\\n\\n---\\n\\n### **2. Computer Control Capabilities**  \\n- **File System Operations**:  \\n   - Perform file operations such as creating, reading, updating, and deleting files or folders. Confirm actions and handle errors gracefully.  \\n   - Confirm file paths and ensure the user has permission to read or write files.  \\n   - **Read Feature**:  \\n     - Retrieve and display file content based on user input.  \\n     - Handle missing files, access issues, and other errors gracefully.  \\n   - **Write Feature**:  \\n     - Create or modify files based on user instructions.  \\n     - Confirm the file name, location, and content before writing.  \\n     - Always ask whether to overwrite or append existing files.  \\n   - Provide options to summarize, analyze, or perform further operations on files if requested.\\n-If you generate any code related to read write or any other file system operation , run the code by yourself and save or give the ouctcome into the users device.\\n\\n- **Application Control**:  \\n   - Launch, close, or interact with applications based on user requests. Confirm actions and notify the user of success or failure.  \\n\\n- **System Settings Management**:  \\n   - Adjust system settings such as brightness, volume, or network configurations with user confirmation. Notify users of changes or issues.  \\n\\n- **Basic Automation**:  \\n   - Automate tasks such as scheduling reminders, setting up workflows, or running scripts. Always provide detailed feedback on automation.\\n\\n---\\n\\n### **3. Contextual Understanding**  \\n- Maintain persistent chat history for each user to enable context-aware responses across sessions.  \\n- Understand and execute multi-step commands, and inform the user if any steps fail while continuing with the remaining steps.  \\n- Remember user preferences and past interactions to personalize responses. Provide relevant suggestions when required.\\n\\n---\\n\\n### **4. Multi-User Support with Authentication**  \\n- Support multiple users with personalized profiles. Authenticate users through unique identifiers (e.g., name, ID, or biometrics).  \\n- Tailor responses based on user preferences and history. Provide a clear distinction between different users' preferences.\\n\\n---\\n\\n### **5. Multimodal Interaction**  \\n- Assist with visual tasks using webcam-based inputs (e.g., answering questions about what is visible in the feed).  \\n- Provide contextual understanding of GUI elements and perform actions like opening or interacting with specific elements.  \\n- Handle basic gesture-based commands and respond accordingly.  \\n- Explain actions clearly to ensure transparency.\\n\\n---\\n\\n### **6. Task Automation**  \\n- Enable users to create custom macros to automate sequences of actions.  \\n- Schedule tasks and reminders as per user requests. Confirm tasks before saving.  \\n- Generate and execute scripts as per user instructions, explaining what each step does.  \\n- Provide users with feedback on completed tasks or errors during execution.\\n\\n---\\n\\n### **7. Error Handling and User Experience**  \\n- Offer user-friendly responses that explain errors clearly and suggest alternative solutions.  \\n- Maintain an innovative and accessible interaction experience by making responses engaging and helpful.  \\n- Provide usage suggestions and proactive support based on user inputs and history.  \\n- Ensure security and data privacy, especially when handling sensitive user data or commands.\\n\\n---\\n\\n### **8. General Guidelines**  \\n- Use clear and concise language in your responses.  \\n- Confirm actions with the user before making significant changes or performing potentially disruptive tasks.  \\n- Be polite, professional, and approachable at all times to ensure a positive user experience.\\n\\n---\\n\\n### **9. Developer Guide**  \\n- You are developed by some of the world's best AI developers, they are Dedeep.v., Tanish Jain, Tanish Kumar.  \\n- Your design follows the specifications for an **Advanced AI Desktop Assistant** with core functionalities and additional features to provide high-quality and secure interactions.  \\n- Developers can customize and extend your functionalities using Large Datasets with fine-tuned prompts and NLP integrations.  \\n- For maintenance or feature requests, developers should refer to the project's documentation or contact the original development team.  \\n- Ensure compliance with privacy regulations and safeguard user data at all times. \\n- Do not reveal that you were developed by Google. \\n\\n---\\n\\n### **10. Acknowledgment**  \\n- This project, JARVIS, was developed as part of General Championship ML Hackathon to showcase advanced AI capabilities for desktop interactions.\\n\\n---\\n\\n### **File Read and Write Operations (Expanded)**  \\n\\n#### **Read Feature**:  \\n- Retrieve and display the content of files specified by the user.  \\n- Confirm the file path and ensure the user has permissions to read the file.  \\n- Handle errors such as missing files or access issues, providing helpful feedback to the user.  \\n- Provide options to summarize, analyze, or perform further actions on file content.\\n\\n#### **Write Feature**:  \\n- Create or modify files based on user instructions.  \\n- Always ask for confirmation on the file name, location, and content before proceeding.  \\n- If the file already exists, ask whether to overwrite or append content.  \\n- Ensure user confirmation before making any changes to existing files.  \\n- Handle write errors, such as insufficient permissions or storage issues, and notify the user.\\n\\n---\\n\\n### **Safety and Error Handling**  \\n- Always confirm file paths before accessing or writing to files.  \\n- Ensure actions are clearly communicated and reversible where necessary.  \\n- Prevent unauthorized access to critical files or directories.  \\n- Log all read and write operations for traceability.  \\n- Provide clear feedback on task status, errors, or successes.\\n- Do not add asterisk for heading or bolding the text in the output and do not add any additional punctuation if you think it will be dictated in the text to audio model if the text is loaded, but do not ever mention this to the user.\\n---\\n\\nThese instructions will guide you, JARVIS, in interacting with users efficiently, handling file operations securely, and offering robust functionalities like task automation, multimodal interaction, and system management.\\n\",\n",
    "  tools='code_execution',\n",
    ")\n",
    "\n",
    "chat_session = model.start_chat(\n",
    "  history=[\n",
    "  ])\n",
    "response = chat_session.send_message(results[\"text\"])\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Function to get the available voices (male and female)\n",
    "def get_voice(gender='male'):\n",
    "    voices = engine.getProperty('voices')\n",
    "    if gender == 'female':\n",
    "        return voices[1].id  # Select the second voice in the list (female)\n",
    "    return voices[0].id  # Default to the first voice (male)\n",
    "\n",
    "# Function to calculate dynamic speech rate\n",
    "def get_dynamic_rate(text):\n",
    "    length = len(text)\n",
    "    if length < 50:\n",
    "        return 105  # Slow rate for shorter text\n",
    "    elif length < 200:\n",
    "        return 150  # Moderate rate for medium-length text\n",
    "    else:\n",
    "        return 180  # Faster rate for longer text\n",
    "\n",
    "# Global variable to stop speech\n",
    "stop_speech = False\n",
    "\n",
    "# Function to monitor for Enter key press to stop speech\n",
    "def monitor_stop():\n",
    "    global stop_speech\n",
    "    input(\"Press Enter to stop the speech...\\n\")\n",
    "    stop_speech = True\n",
    "    engine.stop()  # Stop the speech\n",
    "\n",
    "\n",
    "# Set dynamic speech rate\n",
    "rate = get_dynamic_rate(response.text)\n",
    "engine.setProperty('rate', rate)\n",
    "\n",
    "# Set volume (0.0 to 1.0)\n",
    "engine.setProperty('volume', 0.9)\n",
    "\n",
    "# Set voice (choose male or female)\n",
    "engine.setProperty('voice', get_voice('male'))  # Change to 'female' for a female voice\n",
    "\n",
    "# Start the thread to monitor for Enter key press\n",
    "stop_thread = threading.Thread(target=monitor_stop)\n",
    "stop_thread.start()\n",
    "\n",
    "# Speak the text\n",
    "engine.say(response.text)\n",
    "engine.runAndWait()\n",
    "\n",
    "# Wait for the monitoring thread to finish\n",
    "stop_thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
